{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Model Evaluation (Hands-on with KNN)\n",
        "\n",
        "This notebook practices the evaluation concepts from class with simple and readable code.\n",
        "\n",
        "Main goals:\n",
        "- Build confusion matrices manually.\n",
        "- Compute classification metrics manually (accuracy, precision, sensitivity/recall, specificity, F1).\n",
        "- Understand ROC curve and AUC with thresholds.\n",
        "- Compute regression metrics manually (MAE, MSE, RMSE, R²).\n",
        "- Compare models using `KNeighborsClassifier` and `KNeighborsRegressor`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, load_iris, load_diabetes, make_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "\n",
        "np.set_printoptions(suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Helper functions (manual implementations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_div(num, den):\n",
        "    return num / den if den != 0 else 0.0\n",
        "\n",
        "\n",
        "def train_test_split_manual(X, y, test_size=0.2, seed=42, stratify=False):\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y)\n",
        "\n",
        "    if not stratify:\n",
        "        indices = np.arange(n)\n",
        "        rng.shuffle(indices)\n",
        "\n",
        "        n_test = int(round(test_size * n))\n",
        "        test_idx = indices[:n_test]\n",
        "        train_idx = indices[n_test:]\n",
        "    else:\n",
        "        train_idx = []\n",
        "        test_idx = []\n",
        "\n",
        "        for cls in np.unique(y):\n",
        "            cls_idx = np.where(y == cls)[0]\n",
        "            rng.shuffle(cls_idx)\n",
        "\n",
        "            n_cls_test = int(round(test_size * len(cls_idx)))\n",
        "            test_idx.extend(cls_idx[:n_cls_test])\n",
        "            train_idx.extend(cls_idx[n_cls_test:])\n",
        "\n",
        "        train_idx = np.array(train_idx)\n",
        "        test_idx = np.array(test_idx)\n",
        "\n",
        "        rng.shuffle(train_idx)\n",
        "        rng.shuffle(test_idx)\n",
        "\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "\n",
        "def confusion_matrix_binary_manual(y_true, y_pred, positive_label=1):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    tp = int(np.sum((y_true == positive_label) & (y_pred == positive_label)))\n",
        "    fp = int(np.sum((y_true != positive_label) & (y_pred == positive_label)))\n",
        "    tn = int(np.sum((y_true != positive_label) & (y_pred != positive_label)))\n",
        "    fn = int(np.sum((y_true == positive_label) & (y_pred != positive_label)))\n",
        "\n",
        "    return {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn}\n",
        "\n",
        "\n",
        "def classification_metrics_from_counts(tp, fp, tn, fn):\n",
        "    accuracy = safe_div(tp + tn, tp + fp + tn + fn)\n",
        "    sensitivity = safe_div(tp, tp + fn)  # recall, TPR\n",
        "    specificity = safe_div(tn, tn + fp)  # TNR\n",
        "    precision = safe_div(tp, tp + fp)    # PPV\n",
        "    f1 = safe_div(2 * precision * sensitivity, precision + sensitivity)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'sensitivity_recall_tpr': sensitivity,\n",
        "        'specificity_tnr': specificity,\n",
        "        'precision_ppv': precision,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_binary_predictions(y_true, y_pred, positive_label=1):\n",
        "    counts = confusion_matrix_binary_manual(y_true, y_pred, positive_label=positive_label)\n",
        "    metrics = classification_metrics_from_counts(\n",
        "        counts['TP'], counts['FP'], counts['TN'], counts['FN']\n",
        "    )\n",
        "    return {**counts, **metrics}\n",
        "\n",
        "\n",
        "def confusion_matrix_multiclass_manual(y_true, y_pred, labels):\n",
        "    labels = list(labels)\n",
        "    idx_of = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "    cm = np.zeros((len(labels), len(labels)), dtype=int)\n",
        "\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        cm[idx_of[yt], idx_of[yp]] += 1\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "def per_class_metrics_from_confusion(cm):\n",
        "    n_classes = cm.shape[0]\n",
        "    rows = []\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        tp = cm[i, i]\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        fn = cm[i, :].sum() - tp\n",
        "\n",
        "        precision = safe_div(tp, tp + fp)\n",
        "        recall = safe_div(tp, tp + fn)\n",
        "        f1 = safe_div(2 * precision * recall, precision + recall)\n",
        "\n",
        "        rows.append({\n",
        "            'class_index': i,\n",
        "            'support': int(cm[i, :].sum()),\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def regression_metrics_manual(y_true, y_pred):\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "\n",
        "    error = y_true - y_pred\n",
        "    mae = np.mean(np.abs(error))\n",
        "    mse = np.mean(error ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    y_mean = np.mean(y_true)\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - y_mean) ** 2)\n",
        "    r2 = 1 - safe_div(ss_res, ss_tot)\n",
        "\n",
        "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Binary classification evaluation (confusion matrix + metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset: Breast Cancer (binary classification)\n",
        "data_bc = load_breast_cancer()\n",
        "X_bc = data_bc.data[:, :3] # Use only the first 5 features for a more complex dataset\n",
        "y_bc = data_bc.target\n",
        "\n",
        "print('Dataset size:', len(y_bc))\n",
        "print('Class names:', list(data_bc.target_names))\n",
        "\n",
        "class_counts = pd.Series(y_bc).value_counts().sort_index()\n",
        "class_share = (class_counts / len(y_bc) * 100).round(2)\n",
        "display(pd.DataFrame({\n",
        "    'class_index': class_counts.index,\n",
        "    'count': class_counts.values,\n",
        "    'percentage': class_share.values\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split_manual(\n",
        "    X_bc, y_bc, test_size=0.25, seed=7, stratify=True\n",
        ")\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=7)\n",
        "clf.fit(X_train_bc, y_train_bc)\n",
        "y_pred_bc = clf.predict(X_test_bc)\n",
        "\n",
        "summary = evaluate_binary_predictions(y_test_bc, y_pred_bc, positive_label=1)\n",
        "summary_df = pd.DataFrame([summary]).round(4)\n",
        "display(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_counts = confusion_matrix_binary_manual(y_test_bc, y_pred_bc, positive_label=1)\n",
        "\n",
        "cm_table = pd.DataFrame(\n",
        "    [[cm_counts['TP'], cm_counts['FN']], [cm_counts['FP'], cm_counts['TN']]],\n",
        "    index=['Actual Positive (1)', 'Actual Negative (0)'],\n",
        "    columns=['Pred Positive (1)', 'Pred Negative (0)']\n",
        ")\n",
        "display(cm_table)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(cm_table.values, cmap='Blues')\n",
        "for i in range(cm_table.shape[0]):\n",
        "    for j in range(cm_table.shape[1]):\n",
        "        plt.text(j, i, int(cm_table.iloc[i, j]), ha='center', va='center', color='black')\n",
        "plt.xticks(range(cm_table.shape[1]), cm_table.columns, rotation=20)\n",
        "plt.yticks(range(cm_table.shape[0]), cm_table.index)\n",
        "plt.title('Binary confusion matrix (KNN)')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1\n",
        "Try changing `n_neighbors` (for example: 1, 3, 4, 5, 6, 7, 15).\n",
        "\n",
        "Questions:\n",
        "- Does accuracy always move in the same direction as F1?\n",
        "- Which metric would you prioritize for a medical problem and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Accuracy can be misleading (imbalanced classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_imb, y_imb = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=8,\n",
        "    n_informative=4,\n",
        "    n_redundant=2,\n",
        "    class_sep=.5,\n",
        "    weights=[0.95, 0.05],\n",
        "    flip_y=0.01,\n",
        "    random_state=4\n",
        ")\n",
        "\n",
        "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split_manual(\n",
        "    X_imb, y_imb, test_size=0.30, seed=11, stratify=True\n",
        ")\n",
        "\n",
        "print('Class distribution in test set:')\n",
        "display(pd.Series(y_test_i).value_counts(normalize=True).sort_index().rename('share'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline model: predict all samples as class 0 (majority class)\n",
        "baseline_pred = np.zeros_like(y_test_i)\n",
        "baseline_eval = evaluate_binary_predictions(y_test_i, baseline_pred, positive_label=1)\n",
        "\n",
        "# KNN model\n",
        "knn_imb = KNeighborsClassifier(n_neighbors=7)\n",
        "knn_imb.fit(X_train_i, y_train_i)\n",
        "knn_pred = knn_imb.predict(X_test_i)\n",
        "knn_eval = evaluate_binary_predictions(y_test_i, knn_pred, positive_label=1)\n",
        "\n",
        "comparison = pd.DataFrame([\n",
        "    {'model': 'Always predict 0', **baseline_eval},\n",
        "    {'model': 'KNN (k=7)', **knn_eval}\n",
        "]).round(4)\n",
        "comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2\n",
        "Change the class imbalance in `make_classification` (for example: `weights=[0.90, 0.10]` and `weights=[0.98, 0.02]`).\n",
        "\n",
        "Questions:\n",
        "- What happens to accuracy vs sensitivity (recall)?\n",
        "- Why can high accuracy still be a bad model in imbalanced problems?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ROC curve and AUC (advanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def roc_points_manual(y_true, y_prob, thresholds):\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for t in thresholds:\n",
        "        y_pred_t = (y_prob >= t).astype(int)\n",
        "        c = confusion_matrix_binary_manual(y_true, y_pred_t, positive_label=1)\n",
        "\n",
        "        tpr = safe_div(c['TP'], c['TP'] + c['FN'])\n",
        "        fpr = safe_div(c['FP'], c['FP'] + c['TN'])\n",
        "\n",
        "        rows.append({'threshold': t, 'TPR': tpr, 'FPR': fpr})\n",
        "\n",
        "    roc_df = pd.DataFrame(rows)\n",
        "    return roc_df\n",
        "\n",
        "\n",
        "def auc_manual(fpr, tpr):\n",
        "    order = np.argsort(fpr)\n",
        "    fpr_sorted = np.array(fpr)[order]\n",
        "    tpr_sorted = np.array(tpr)[order]\n",
        "    return np.trapezoid(tpr_sorted, fpr_sorted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds = np.linspace(1.0, 0.0, 51)\n",
        "\n",
        "roc_rows = []\n",
        "curves = {}\n",
        "\n",
        "for k in [3, 15]:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train_i, y_train_i)\n",
        "\n",
        "    # Probability of positive class (class 1)\n",
        "    y_prob = model.predict_proba(X_test_i)[:, 1]\n",
        "\n",
        "    roc_df = roc_points_manual(y_test_i, y_prob, thresholds)\n",
        "    auc_value = auc_manual(roc_df['FPR'], roc_df['TPR'])\n",
        "\n",
        "    curves[k] = roc_df\n",
        "    roc_rows.append({'k': k, 'AUC_manual': round(auc_value, 4)})\n",
        "\n",
        "pd.DataFrame(roc_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "for k, roc_df in curves.items():\n",
        "    auc_value = auc_manual(roc_df['FPR'], roc_df['TPR'])\n",
        "    plt.plot(roc_df['FPR'], roc_df['TPR'], marker='o', markersize=3, label=f'k={k} (AUC={auc_value:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], '--', color='gray', label='random model')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Manual ROC curves (KNN)')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3\n",
        "Try different `k` values and a different number of thresholds.\n",
        "\n",
        "Questions:\n",
        "- Which model has better AUC?\n",
        "- Why do we need many thresholds for ROC/AUC?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Multiclass evaluation (Iris confusion matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X_iris = iris.data[:, :2]  # Use only the first 2 features for a more complex dataset\n",
        "y_iris = iris.target\n",
        "label_names = iris.target_names\n",
        "\n",
        "X_train_ir, X_test_ir, y_train_ir, y_test_ir = train_test_split_manual(\n",
        "    X_iris, y_iris, test_size=0.25, seed=5, stratify=True\n",
        ")\n",
        "\n",
        "knn_ir = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_ir.fit(X_train_ir, y_train_ir)\n",
        "y_pred_ir = knn_ir.predict(X_test_ir)\n",
        "\n",
        "labels = [0, 1, 2]\n",
        "cm_multi = confusion_matrix_multiclass_manual(y_test_ir, y_pred_ir, labels=labels)\n",
        "\n",
        "cm_df = pd.DataFrame(cm_multi, index=label_names, columns=label_names)\n",
        "display(cm_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(cm_df.values, cmap='Blues')\n",
        "for i in range(cm_df.shape[0]):\n",
        "    for j in range(cm_df.shape[1]):\n",
        "        plt.text(j, i, int(cm_df.iloc[i, j]), ha='center', va='center', color='black')\n",
        "plt.xticks(range(cm_df.shape[1]), cm_df.columns)\n",
        "plt.yticks(range(cm_df.shape[0]), cm_df.index)\n",
        "plt.title('Multiclass confusion matrix (Iris, KNN)')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.ylabel('Actual class')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "per_class_df = per_class_metrics_from_confusion(cm_multi)\n",
        "per_class_df['class_name'] = label_names\n",
        "display(per_class_df[['class_name', 'support', 'precision', 'recall', 'f1_score']].round(4))\n",
        "\n",
        "macro_f1 = per_class_df['f1_score'].mean()\n",
        "overall_accuracy = np.trace(cm_multi) / cm_multi.sum()\n",
        "print('Overall accuracy:', round(overall_accuracy, 4))\n",
        "print('Macro F1       :', round(macro_f1, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4\n",
        "Change `n_neighbors` and `seed` in the Iris section.\n",
        "\n",
        "Questions:\n",
        "- Which class is hardest to classify?\n",
        "- Is overall accuracy enough to understand class-level performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Regression evaluation (MAE, MSE, RMSE, R²)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diabetes = load_diabetes()\n",
        "X_reg = diabetes.data\n",
        "y_reg = diabetes.target\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split_manual(\n",
        "    X_reg, y_reg, test_size=0.25, seed=9, stratify=False\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for k in [1, 3, 5, 11, 25, 51]:\n",
        "    reg = KNeighborsRegressor(n_neighbors=k)\n",
        "    reg.fit(X_train_r, y_train_r)\n",
        "\n",
        "    pred = reg.predict(X_test_r)\n",
        "    metrics = regression_metrics_manual(y_test_r, pred)\n",
        "\n",
        "    rows.append({'k': k, **metrics})\n",
        "\n",
        "reg_df = pd.DataFrame(rows).sort_values('RMSE')\n",
        "reg_df.round(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_k = int(reg_df.iloc[0]['k'])\n",
        "print('Best k by RMSE:', best_k)\n",
        "\n",
        "best_reg = KNeighborsRegressor(n_neighbors=best_k)\n",
        "best_reg.fit(X_train_r, y_train_r)\n",
        "best_pred = best_reg.predict(X_test_r)\n",
        "\n",
        "best_metrics = regression_metrics_manual(y_test_r, best_pred)\n",
        "print(pd.Series(best_metrics).round(4))\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test_r, best_pred, alpha=0.7)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "plt.title(f'Diabetes regression (KNN, k={best_k})')\n",
        "\n",
        "min_v = min(y_test_r.min(), best_pred.min())\n",
        "max_v = max(y_test_r.max(), best_pred.max())\n",
        "plt.plot([min_v, max_v], [min_v, max_v], '--', color='red')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5\n",
        "- Try a different split (`seed` and `test_size`).\n",
        "- Compare model choice when selecting by MAE vs RMSE vs R².\n",
        "\n",
        "Questions:\n",
        "- Which metric penalizes large errors more?\n",
        "- Can two models have similar RMSE but different R²?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusions\n",
        "- Evaluation metrics help compare models objectively.\n",
        "- Accuracy alone is often insufficient, especially with class imbalance.\n",
        "- Confusion matrix + precision/recall/F1 gives better detail for classification.\n",
        "- ROC/AUC summarizes behavior across multiple thresholds.\n",
        "- For regression, choose MAE/MSE/RMSE/R² based on the type of error you care about."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7c16d9",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
