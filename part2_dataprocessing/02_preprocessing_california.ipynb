{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing (California Housing)\n",
    "\n",
    "This notebook follows the class notes on data preprocessing and applies them to the **California Housing** dataset. We will:\n",
    "- Inspect data format\n",
    "- Identify and correct erroneous values\n",
    "- Detect and treat outliers\n",
    "- Normalize numerical attributes\n",
    "- Disaggregate categorical variables (after creating a categorical feature)\n",
    "- Categorize numerical variables\n",
    "\n",
    "All preprocessing steps are explicit, justified, and documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c200be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1279ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "cal = fetch_california_housing(as_frame=True)\n",
    "cal_data = cal.frame.copy()\n",
    "cal_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472745e3",
   "metadata": {},
   "source": [
    "## Create a Dirty Version of the Dataset\n",
    "\n",
    "To practice preprocessing, we will intentionally introduce **erroneous values** (e.g., \"NA\") and **outliers** into a copy of the dataset. We keep the original data intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dirty copy (deterministic for reproducibility)\n",
    "cal_dirty = cal_data.copy()\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Identify candidate columns\n",
    "num_cols = cal_dirty.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Erroneous value tokens (common in real datasets)\n",
    "err_tokens = [\"NA\", \"N/A\", \"?\", \"\", \" \", \"nan\", \"NaN\", \"None\", \"null\", \"NULL\", \"-\"]\n",
    "\n",
    "# 1) Introduce erroneous values across multiple numeric columns\n",
    "if num_cols:\n",
    "    n_err = max(1, int(0.02 * len(cal_dirty)))  # 2% of rows\n",
    "    for col in num_cols[:3]:  # affect first 3 numeric columns\n",
    "        rows = rng.choice(cal_dirty.index, size=n_err, replace=False)\n",
    "        cal_dirty.loc[rows, col] = rng.choice(err_tokens, size=n_err)\n",
    "\n",
    "# 2) Introduce missing values (np.nan) in more numeric columns\n",
    "if num_cols:\n",
    "    n_miss = max(1, int(0.015 * len(cal_dirty)))  # 1.5% of rows\n",
    "    for col in num_cols[3:6]:  # next 3 numeric columns (if any)\n",
    "        rows = rng.choice(cal_dirty.index, size=n_miss, replace=False)\n",
    "        cal_dirty.loc[rows, col] = np.nan\n",
    "\n",
    "# 3) Introduce outliers in multiple numeric columns\n",
    "if num_cols:\n",
    "    n_out = max(1, int(0.005 * len(cal_dirty)))  # 0.5% of rows\n",
    "    for col in num_cols[:4]:  # outliers in first 4 numeric columns\n",
    "        rows = rng.choice(cal_dirty.index, size=n_out, replace=False)\n",
    "        numeric_series = pd.to_numeric(cal_dirty[col], errors='coerce')\n",
    "        max_val = numeric_series.max()\n",
    "        cal_dirty.loc[rows, col] = max_val * rng.integers(20, 60, size=n_out)\n",
    "\n",
    "# Quick check\n",
    "print(\"Original missing values:\")\n",
    "print(cal_data.isna().sum().head())\n",
    "print(\"Dirty missing values (after injection):\")\n",
    "print(cal_dirty.isna().sum().head())\n",
    "\n",
    "cal_dirty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44d5d1",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "Data preprocessing is the set of transformations required to make data suitable for machine learning. It is necessary but must be done carefully to avoid introducing bias or errors.\n",
    "\n",
    "Key principle: **all transformations must be explicit, justified, and documented**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c32aec",
   "metadata": {},
   "source": [
    "## 2. Format\n",
    "\n",
    "A dataset is a collection of data points (rows) described by attributes (columns). Attributes may be:\n",
    "- Categorical (finite set of values)\n",
    "- Numerical (continuous values)\n",
    "- Text (strings)\n",
    "- Other (images, sounds, dates, etc.)\n",
    "\n",
    "We first inspect the dataset format and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic inspection\n",
    "print(cal_data.shape)\n",
    "display(cal_data.head())\n",
    "\n",
    "# Data types and missing values overview\n",
    "cal_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate columns by type\n",
    "num_cols = cal_data.select_dtypes(include='number').columns.tolist()\n",
    "cat_cols = cal_data.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "print(\"Numerical columns:\", num_cols)\n",
    "print(\"Categorical/Text columns:\", cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f2d0f",
   "metadata": {},
   "source": [
    "### 2.2 Data Format considerations\n",
    "\n",
    "We need to consider:\n",
    "- Character encoding (e.g., special characters like Ã±)\n",
    "- Numbers (decimal/thousand separators, units)\n",
    "- Categorical values (consistent spelling/casing)\n",
    "- Erroneous values (tokens that represent missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012238b",
   "metadata": {},
   "source": [
    "## 3. Erroneous Values\n",
    "\n",
    "Erroneous values are invalid for an attribute (not outliers). Typical causes:\n",
    "- Missing values\n",
    "- Incorrect format (e.g., \"30,0\" instead of \"30.0\")\n",
    "- Measurement errors\n",
    "- Encoding errors\n",
    "- Nonexistent categories\n",
    "\n",
    "We first detect common missing-value tokens and format issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f575031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common missing-value tokens observed in real datasets\n",
    "missing_tokens = [\"NA\", \"N/A\", \"?\", \"\", \" \", \"nan\", \"NaN\", \"None\", \"null\", \"NULL\", \"-\"]\n",
    "\n",
    "# Replace tokens with NaN for consistent handling\n",
    "clean = cal_dirty.replace(missing_tokens, np.nan)\n",
    "\n",
    "# Coerce numeric columns to numeric (invalid strings become NaN)\n",
    "for col in num_cols:\n",
    "    clean[col] = pd.to_numeric(clean[col], errors='coerce')\n",
    "\n",
    "missing_counts = clean.isna().sum().sort_values(ascending=False)\n",
    "missing_counts[missing_counts > 0].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c3ad5",
   "metadata": {},
   "source": [
    "### 3.5 Correction strategies\n",
    "\n",
    "Typical strategies:\n",
    "- Remove rows or columns with erroneous values\n",
    "- Correct values when the fix is unambiguous\n",
    "- Impute missing values (mean, median, mode, fixed value, or a model)\n",
    "\n",
    "Below we show a simple imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94820742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple imputation example\n",
    "imputed = clean.copy()\n",
    "\n",
    "# Numerical: median (robust to outliers)\n",
    "for col in num_cols:\n",
    "    if imputed[col].isna().any():\n",
    "        # Your code here (use the median() and fillna() methods)\n",
    "        imputed[col] = ...\n",
    "\n",
    "imputed.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c9b10",
   "metadata": {},
   "source": [
    "## 4. Outliers\n",
    "\n",
    "Outliers are values that deviate significantly from the distribution. They can strongly affect models.\n",
    "\n",
    "Detection is subjective and context-dependent. Common rules:\n",
    "- More than N standard deviations from the mean (e.g., N=3)\n",
    "- Above or below percentile P (e.g., P=95)\n",
    "- Low probability under the distribution (e.g., p < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a numerical column to illustrate outlier detection\n",
    "num_col = num_cols[0]\n",
    "\n",
    "# Z-score and IQR detection for the chosen column\n",
    "x = imputed[num_col]\n",
    "\n",
    "# Z-score method (standardize and identify outliers as samples with |z| > 3)\n",
    "outliers_z = ...\n",
    "\n",
    "# IQR method\n",
    "q1, q3 = x.quantile(0.25), x.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "outliers_iqr = imputed[(x < lower) | (x > upper)]\n",
    "\n",
    "print(f\"Column: {num_col}\")\n",
    "print(\"Outliers (Z-score > 3):\", len(outliers_z))\n",
    "print(\"Outliers (IQR rule):\", len(outliers_iqr))\n",
    "\n",
    "outliers_iqr[[num_col]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5580d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional visualization with two numerical features\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(imputed[num_cols[0]], imputed[num_cols[1]], alpha=0.5)\n",
    "plt.xlabel(num_cols[0])\n",
    "plt.ylabel(num_cols[1])\n",
    "plt.title(\"Scatter plot (possible outliers)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927236fc",
   "metadata": {},
   "source": [
    "### 4.3 Treatment\n",
    "\n",
    "Outlier treatment is often similar to erroneous values:\n",
    "- Remove\n",
    "- Correct (if clearly wrong)\n",
    "- Impute or cap (winsorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469da991",
   "metadata": {},
   "source": [
    "## 5. Normalization\n",
    "\n",
    "Normalization transforms numerical data to satisfy specific properties:\n",
    "- Range (e.g., [0, 1])\n",
    "- Unit (consistent measurement units)\n",
    "- Scale (attributes comparable for distance-based models)\n",
    "\n",
    "We use two common techniques: **standardization** and **scaling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (mean 0, std 1) and Min-Max scaling [0,1]\n",
    "# HINT: Use the formula (x - mean) / std for standardization and (x - min) / (max - min) for scaling\n",
    "normalized = imputed.copy()\n",
    "\n",
    "# Standardization\n",
    "normalized[num_cols] = ...\n",
    "\n",
    "# Scaling\n",
    "scaled = imputed.copy()\n",
    "scaled[num_cols] = ...\n",
    "\n",
    "normalized[num_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a0b0e",
   "metadata": {},
   "source": [
    "## 6. Disaggregation\n",
    "\n",
    "Disaggregation converts categorical attributes into numerical features so they can be used by ML algorithms.\n",
    "\n",
    "The California Housing dataset is fully numeric, so we will create a categorical feature for demonstration (e.g., binning `MedInc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2961c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a categorical feature from a numeric one\n",
    "cal_cat = imputed.copy()\n",
    "cal_cat[\"MedInc_bin\"] = pd.qcut(cal_cat[\"MedInc\"], q=4, labels=[\"low\", \"mid\", \"high\", \"very_high\"])\n",
    "\n",
    "# Label encoding\n",
    "cal_cat[\"MedInc_label\"] = cal_cat[\"MedInc_bin\"].astype('category').cat.codes\n",
    "cal_cat[[\"MedInc_bin\", \"MedInc_label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "medinc_onehot = pd.get_dummies(cal_cat[\"MedInc_bin\"], prefix=\"MedInc\")\n",
    "medinc_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea293644",
   "metadata": {},
   "source": [
    "## 7. Categorization\n",
    "\n",
    "Categorization converts numerical attributes into categorical ones. This is subjective and context-dependent.\n",
    "\n",
    "Example: assigning house-age into bins using the [pd.cut()](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorization example\n",
    "categorized = imputed.copy()\n",
    "\n",
    "categorized[\"HouseAge_group\"] = pd.cut(...)\n",
    "\n",
    "categorized[[\"HouseAge\", \"HouseAge_group\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "- The dataset is a fundamental part of the ML process.\n",
    "- Preprocessing is required and must be explicit, justified, and documented.\n",
    "- Error correction includes removing, correcting, or imputing values.\n",
    "- Outlier treatment includes detecting and then removing, correcting, or imputing.\n",
    "- Normalization uses standardization or scaling.\n",
    "- Disaggregation uses labeling or one-hot encoding.\n",
    "- Categorization converts numerical values into categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
